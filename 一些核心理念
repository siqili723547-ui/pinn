偏导数
链式法则 在反向传播时层层反推参数对最后结果的影响程度
梯度  前向传播收集loss数据 量化每个参数对总损失的贡献程度，从而为优化器指明通往最小损失方向的道路
张量 requires_grad=True 属性的含义：追踪标签
矩阵乘法 理解为集成乘法 一次把一类数据与另一类数据分别相乘
ODE与PDE的区别 常积分和偏积分 偏积分为主 可以分析多个变量中单一变量变化对于结果的影响
ICs和BCs的物理和数学意义 起始条件与边界条件 作为硬约束让求解框在一定范围内
NumPy

Numpy的核心操作口诀
记住这几个关键词，你就掌握了Numpy的精髓：
1. np.array() - 创建：从列表等数据创建Numpy数组。
2. shape - 查看：查看数组的维度（例如 (5000, 2)）。
3. reshape - 变形：改变数组的形状而不改变数据内容。
4. slicing - 切片：获取数组的子集（例如 points[0:10] 取前10行）。
5. 矢量化运算 - 计算：对整个数组进行加减乘除、函数计算（如 np.sin(arr)），避免使用循环。
6. np.linspace / np.random - 生成：生成用于采样和测试的序列或随机数据。

Matplotlib/Plotly 帮助画图使数据可视化的工具
网络构建 (torch.nn.Module) 掌握如何构建一个标准的多层感知机(MLP)。
自动微分引擎 (torch.autograd.grad)必须精通如何调用它来计算网络输出对输入的导数，这是构建 Lphy 的实践基础。
优化器 (torch.optim): 理解 Adam 优化器的作用。
训练循环: 能够独立编写标准的“前向传播 → 计算损失 → 反向传播 → 更新权重”的流程。
多层感知机 (MLP): 它是 PINN 中最基础、最核心的“解的容器”或函数逼近器。理解为多层分析数据来辅助做出决策
激活函数: 重点理解 tanh 和 sin 为何在 PINN 问题中备受青睐，因
为它们是无限次可导的 (C∞)，高阶导数平滑，有利于拟合复杂的
物理场。
损失函数: 深刻理解其作为模型“目标函数”或“价值体系”的本
质。在 PINN 中，它不再是单一的数据拟合误差，而是数据保真度
和物理一致性的“加权妥协”。
优化器: 将其理解为一位“勤奋的求解者”，它的任务就是根据损失
函数提供的“地图”（梯度），不知疲倦地去寻找参数空间中的“最
低谷”。更新权重是它的工作
