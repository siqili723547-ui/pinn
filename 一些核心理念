偏导数 固定其他变量，只看一个变量的变化所造成的影响
链式法则 在反向传播时层层反推参数对最后结果的影响程度
梯度  前向传播收集loss数据 量化每个参数对总损失的贡献程度，从而为优化器指明通往最小损失方向的道路
张量 是深度学习中数据的标准容器。它可以看作是标量、向量、矩阵向更高维度的推广。
requires_grad=True 属性的含义：追踪标签
loss = model(x) 计算损失
loss.backward() 反向传播
optimizer.step() 更新参数
矩阵乘法 可以理解为集成乘法 一次把一类数据与另一类数据分别相乘
ODE与PDE的区别 常积分和偏积分 偏积分为主 可以分析多个变量中单一变量变化对于结果的影响
ICs和BCs的物理和数学意义 起始条件与边界条件 作为硬约束让求解框在一定范围内
NumPy 强大的工具箱 有自动微分功能

Numpy的核心操作口诀
记住这几个关键词，你就掌握了Numpy的精髓：
1. np.array() - 创建：从列表等数据创建Numpy数组。
2. shape - 查看：查看数组的维度（例如 (5000, 2)）。
3. reshape - 变形：改变数组的形状而不改变数据内容。
4. slicing - 切片：获取数组的子集（例如 points[0:10] 取前10行）。
5. 矢量化运算 - 计算：对整个数组进行加减乘除、函数计算（如 np.sin(arr)），避免使用循环。
6. np.linspace / np.random - 生成：生成用于采样和测试的序列或随机数据。

Matplotlib/Plotly 帮助画图 使数据可视化的工具
训练循环: 前向传播 → 计算损失 → 反向传播 → 更新权重 然后下一轮循环
多层感知机 (MLP): 它是是一个经典的、由全连接层堆叠而成的神经网络 是PINN 中最基础、最核心的“解的容器”或函数逼近器，可以理解为多层分析数据来辅助做出决策
激活函数: 无限次可导的 (C∞)，高阶导数平滑，其稳定可控性有利于拟合复杂的物理场
损失函数: 简单来说就是通过损失函数的变化来确定神经网络的调整方向
优化器: 更新权重是它的工作，前提是要把所有特征都转换到同一个尺度和基准上，让它们能够被公平地比较和处理。
Adam优化器是一个自适应、高效率的优化算法，它通过结合动量（一阶矩）和自适应学习率（二阶矩）来加速收敛并提高稳定性。
热传导方程一维 ∂u/∂t = α * ∂²u/∂x²
一个点升温或降温的速度 (∂u/∂t)，正比于 (α) 它和自己周围环境平均温度的差距 (∂²u/∂x²)。
这里的 α (alpha) 叫做 热扩散率，是材料本身的属性。金属的 α 大，热量传得快；木头的 α 小，热量传得慢。
配置点 挑选出有限数量的、有代表性的点，然后要求神经网络在这些点上遵守物理定律。
物理损失和数据损失 可以理解为模型与已知数据点和物理定律的偏差
权重矩阵 可以理解为可调节的配重 通过反复调整使它满足最小损失
偏置向量 可以理解调整这个模型的敏感度
